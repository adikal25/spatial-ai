# üîç Self-Correcting Vision-Language QA with GPT-5 Nano

An automated verification and self-correction pipeline using **GPT-5 Nano** that addresses spatial reasoning hallucinations through depth geometry and explicit self-reasoning loops.

## üéØ Overview

Vision-Language Models (VLMs) often hallucinate about object sizes, distances, and counts, contradicting basic spatial geometry. This project implements a three-stage pipeline with **GPT-5 Nano's self-reasoning capabilities**:

1. **Ask** (1-3s): GPT-5 Nano generates initial response with bounding boxes and reasoning
2. **Verify** (1-4s): Depth estimation + geometric contradiction detection
3. **Correct** (1-4s): GPT-5 Nano engages in explicit self-reflection and correction

## ‚ú® Key Features

- **GPT-5 Nano-Powered**: Uses GPT-5 Nano with vision capabilities and tool use
- **Self-Reasoning Loop**: GPT-5 Nano explicitly reflects on its mistakes and corrects them
- **Automated Verification**: Uses MiDaS depth estimation to validate spatial claims
- **Transparent Reasoning**: See GPT-5 Nano's internal reasoning and self-reflection
- **Real-time Processing**: Target latency <8s end-to-end
- **Visual Proof**: Generates proof overlays with depth maps and annotations
- **REST API**: FastAPI backend for easy integration
- **Interactive Demo**: Streamlit web interface

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     User Input                              ‚îÇ
‚îÇ              (Image + Spatial Question)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 1: ASK                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ GPT-5 Nano with Vision                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Tool use for structured bounding boxes             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Initial spatial reasoning                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Explicit reasoning trace                           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Answer + Reasoning + Bounding Boxes
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 2: VERIFY                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Depth Estimation (MiDaS)                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Generate depth map                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Extract object depths                              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Geometric Verifier                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Detect size contradictions                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Detect distance contradictions                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Detect count contradictions                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Generate proof overlay                             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Contradictions + Proof Image
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 3: SELF-CORRECTION LOOP (if contradictions found)    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ GPT-5 Nano Self-Reasoning Process                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ 1. Review: Re-examine original image                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ 2. Analyze: Study depth visualization                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ 3. Evaluate: Compare reasoning vs evidence           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ 4. Reflect: Identify errors made                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ 5. Correct: Provide revised answer                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Explicit self-reflection                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Honest error acknowledgment                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ - Confidence score                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Final Output                                   ‚îÇ
‚îÇ  - Original Answer + Reasoning                              ‚îÇ
‚îÇ  - Revised Answer (if corrected)                            ‚îÇ
‚îÇ  - Self-Reflection                                          ‚îÇ
‚îÇ  - Confidence Score                                         ‚îÇ
‚îÇ  - Proof Overlay                                            ‚îÇ
‚îÇ  - Spatial Metrics                                          ‚îÇ
‚îÇ  - Performance Stats                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Requirements

- Python 3.11+
- **OpenAI API key** (for GPT-5 Nano)
- (Optional) GPU for faster depth estimation

## üöÄ Quick Start

### Simple Demo (Recommended)

1. **Clone/navigate to the project**
```bash
cd self-correcting-vlm-qa
```

2. **Run setup script**
```bash
./setup.sh
```

3. **Add your OpenAI API key**
```bash
# Edit config/.env and add your key
OPENAI_API_KEY=your_openai_api_key_here
```

4. **Run the demo!**
```bash
./run_demo.sh
```

The demo will open in your browser at http://localhost:8501

**That's it!** Upload an image and ask spatial questions.

### Manual Setup (Alternative)

If you prefer manual setup:

1. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

2. **Set up environment**
```bash
cp config/.env.example config/.env
# Edit config/.env and add your OpenAI API key
```

3. **Run API (Terminal 1)**
```bash
python -m uvicorn src.api.main:app --reload
```

4. **Run Demo (Terminal 2)**
```bash
streamlit run demo/app.py
```

## üîß Configuration

Edit `config/.env` to customize:

```env
# API Keys
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI Configuration
OPENAI_VLM_MODEL=gpt-4o
OPENAI_TEMPERATURE=0.2
OPENAI_MAX_OUTPUT_TOKENS=2048

# Depth Model Configuration
DEPTH_MODEL=midas_v3_small
# Options: midas_v3_small, midas_v3_dpt_large

# Performance Settings
MAX_IMAGE_SIZE=1024
ENABLE_GPU=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Logging
LOG_LEVEL=INFO
```

## üì° API Usage

### Health Check

```bash
curl http://localhost:8000/health
```

### Ask Question

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "image": "base64_encoded_image_data",
    "question": "Which object is closer to the camera?",
    "use_fallback": false
  }'
```

### Response Format

```json
{
  "answer": "The car is closer to the camera.",
  "revised_answer": "Actually, based on depth analysis, the tree is closer.",
  "confidence": 0.85,
  "proof_overlay": "data:image/png;base64,...",
  "detected_objects": [
    {
      "x1": 0.1,
      "y1": 0.2,
      "x2": 0.5,
      "y2": 0.8,
      "label": "car",
      "confidence": 0.9
    }
  ],
  "spatial_metrics": [
    {
      "object_id": "obj_0_car",
      "depth_mean": 45.2,
      "depth_std": 3.1,
      "estimated_distance": 4.52
    }
  ],
  "contradictions": [
    {
      "type": "distance",
      "claim": "Car is closer",
      "evidence": "Tree has lower depth value (32.1 vs 45.2)",
      "severity": 0.7
    }
  ],
  "latency_ms": {
    "ask_ms": 2100,
    "verify_ms": 1800,
    "correct_ms": 1500,
    "total_ms": 5400
  }
}
```

## üß™ Testing

Run tests with pytest:

```bash
pytest tests/
```

## üìä Performance Targets

| Metric | Gold | Silver | Bronze |
|--------|------|--------|--------|
| Total Latency | <4s | <8s | <12s |
| Accuracy Improvement | +35pp | +25pp | +15pp |
| Code Complexity | <800 LOC | <1200 LOC | <2000 LOC |

## üèóÔ∏è Project Structure

```
self-correcting-vlm-qa/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vlm_service.py          # VLM interaction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ depth_service.py        # Depth estimation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ verifier_service.py     # Contradiction detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ correction_service.py   # Self-correction logic
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py              # Pydantic models
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ demo/
‚îÇ   ‚îî‚îÄ‚îÄ app.py                      # Streamlit demo
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## üîç How It Works

### 1. Initial GPT-5 Nano Query (Ask Stage)

The system queries **GPT-5 Nano** with the user's spatial question and image. GPT-5 Nano responds with:
- Natural language answer
- Internal reasoning about spatial relationships
- Bounding boxes for detected objects (via tool use)

### 2. Geometric Verification (Verify Stage)

The verifier:
1. Uses MiDaS to generate a depth map
2. Extracts depth values for each bounding box
3. Computes spatial metrics (mean depth, size, estimated distance)
4. Compares metrics against VLM claims
5. Detects contradictions in:
   - **Relative distances**: "Object A is closer than B" vs depth values
   - **Relative sizes**: "Same size" vs bounding box areas
   - **Object counts**: "3 cars" vs detected objects
6. Generates proof overlay with side-by-side comparison

### 3. Self-Correction with Reasoning Loop (Correct Stage)

If contradictions are found, **GPT-5 Nano engages in explicit self-reasoning**:
1. GPT-5 Nano receives:
   - Original image
   - Depth visualization proof overlay
   - Its original answer and reasoning
   - Detailed contradictions with geometric evidence

2. GPT-5 Nano follows a structured self-reflection process:
   - **Review**: Re-examines the original image
   - **Analyze**: Studies the depth map visualization
   - **Evaluate**: Compares its reasoning against geometric measurements
   - **Reflect**: Explicitly identifies where it went wrong
   - **Correct**: Provides revised answer with honest error acknowledgment

3. GPT-5 Nano outputs:
   - Self-reflection explaining its thought process
   - Revised answer (or reaffirmation if evidence is inconclusive)
   - Confidence score (0-1)

## üé® Example Use Cases

- **Autonomous vehicles**: Verify object distance estimates
- **Robotics**: Validate spatial reasoning for manipulation
- **Accessibility**: Describe scene layouts accurately
- **Education**: Teach spatial reasoning with feedback
- **Research**: Study VLM spatial understanding limitations

## ü§ù Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## üìù License

MIT License - see LICENSE file for details

## üôè Acknowledgments

- **OpenAI**: GPT-5 Nano with vision capabilities and self-reasoning
- **MiDaS**: Intel ISL for depth estimation
- **FastAPI**: Web framework
- **Streamlit**: Demo UI framework

## üìö References

- [MiDaS: Monocular Depth Estimation](https://github.com/isl-org/MiDaS)
- [OpenAI GPT Models](https://platform.openai.com/docs/models)
- [OpenAI API Documentation](https://platform.openai.com/docs/overview)

## üêõ Known Limitations

- Depth estimation accuracy depends on monocular depth model limitations
- Contradiction detection uses heuristics; may miss complex cases
- Requires good lighting and clear object boundaries
- Performance depends on VLM API latency

## üó∫Ô∏è Roadmap

- [ ] Support for more depth models (ZoeDepth, DepthAnything)
- [ ] Advanced NLP for better contradiction detection
- [ ] Multi-turn conversation support
- [ ] Fine-tuned VLM for spatial reasoning
- [ ] Batch processing support
- [ ] Metrics dashboard
- [ ] A/B testing framework

---

**Built with ‚ù§Ô∏è for accurate spatial reasoning using GPT-5 Nano's self-correction capabilities**
